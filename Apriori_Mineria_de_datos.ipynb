{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apriori - Mineria_de_datos.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDOtku-OccZ-"
      },
      "source": [
        "NOMBRE: ALEX HELDER HUANCARA CCOLQQUE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvOxHQSwm56I"
      },
      "source": [
        "Importamos las librerías a usar "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywIukrDim14c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7Uu3lXznoUB"
      },
      "source": [
        "Usaremos esta funcion para crear combinaciones de una entrada que nos devuelve subsecuencias de longitud r, si los elementos de la entrada son únicos, no habrá valores repetidos en las combinaciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGmc8uqPm1XZ"
      },
      "source": [
        "def combinaciones(entrada, long_salida):\n",
        "    aux = tuple(entrada)\n",
        "    n = len(aux)\n",
        "    if long_salida > n:\n",
        "        return\n",
        "    indices = list(range(long_salida))\n",
        "    yield tuple(aux[i] for i in indices)\n",
        "\n",
        "    while True:\n",
        "        for i in reversed(range(long_salida)):\n",
        "            if indices[i] != i + n - long_salida:\n",
        "                break\n",
        "        else:\n",
        "            return\n",
        "        indices[i] += 1\n",
        "        for j in range(i + 1, long_salida):\n",
        "            indices[j] = indices[j - 1] + 1\n",
        "        yield tuple(aux[i] for i in indices)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfSbAdTztodL"
      },
      "source": [
        "Prueba de las combinaciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQUapzx4nuRA",
        "outputId": "0d16219a-0881-49a0-d50a-a95518888c77"
      },
      "source": [
        "tupaaa = combinaciones('abcde', 3)\n",
        "for x in tupaaa:\n",
        "  print(x)\n",
        "  "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('a', 'b', 'c')\n",
            "('a', 'b', 'd')\n",
            "('a', 'b', 'e')\n",
            "('a', 'c', 'd')\n",
            "('a', 'c', 'e')\n",
            "('a', 'd', 'e')\n",
            "('b', 'c', 'd')\n",
            "('b', 'c', 'e')\n",
            "('b', 'd', 'e')\n",
            "('c', 'd', 'e')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3gFjVAdtsdP"
      },
      "source": [
        "# Implementacion del algoritmo Apriori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1Uza5EUuBvD"
      },
      "source": [
        "class Apriori:\n",
        "  # inilcializacion de atributos\n",
        "  def __init__(self, datos,  min_support = 0, confidence = 0, lift = 0):\n",
        "    self.datos = datos\n",
        "    self.min_support = min_support\n",
        "    self.confidence = confidence\n",
        "    self.lift = lift\n",
        "\n",
        "  \"\"\" Esta funcion nos permite tranformar el dataset.npy en una lista, además\n",
        "  elimina las canciones repetidas (transformacion en conjunto) y ademas\n",
        "  almacenamos en una variable las veces que una cancion se repite dentro de la playlist\"\"\"\n",
        "  def preparar_datos(self):\n",
        "    self.playlists = list(self.datos.item().values()) # convertir en lista\n",
        "    self.playlists = [set(playlist) for playlist in self.playlists] # convertimos en conjunto para eliminar musicas repetidas\n",
        "    canciones = [item for sublista in self.playlists for item in sublista]\n",
        "    self.songs_counter = pd.Series(data=canciones).value_counts().to_dict()\n",
        "\n",
        "  \"\"\"Con este método generamos un diccionario con las canciones y los indices \n",
        "  de las listas\"\"\"\n",
        "  import collections\n",
        "  def obtener_apariciones_canciones(self):\n",
        "    canciones_en_playlists = collections.defaultdict(set) # Creamos un dicionario de tuplas\n",
        "    for indice, playlist in enumerate(self.playlists): #recorremos las playlists\n",
        "      for cancion in playlist: #Para cada cancion agregamos sus indices en el diccionario\n",
        "          canciones_en_playlists[cancion].add(indice)\n",
        "    self.songs_in_playlists = canciones_en_playlists\n",
        "\n",
        "  \"\"\" Esta funcion nos genera itemset frecuentes largos, aquellos itemsets que \n",
        "  que aparecen mas veces que el min_support\"\"\"\n",
        "  def generate_L_1(self):\n",
        "    #verificamos que se agreguen canciones que sobrepasen el min_support\n",
        "    self.L_1_counter = { cancion: tiempos for cancion, tiempos in self.songs_counter.items() if tiempos / len(self.playlists) >= self.min_support }\n",
        "    self.L_1 = [{cancion} for cancion in self.L_1_counter.keys()]\n",
        "\n",
        "\n",
        "  \"\"\"Esta funcion nos generará posibles itemsets frecuentes, para eso usaremos \n",
        "  la union de conjuntos\"\"\"\n",
        "  def posibles_itemsets_frecuentes(self, current_itemsets, k):\n",
        "    C_k = set()\n",
        "    m = k - 2\n",
        "    for candidato in current_itemsets:\n",
        "      candidato = list(candidato) # convertimos en lista los posibles items sets frecuentes\n",
        "      for aux_candidate in current_itemsets:\n",
        "        aux_candidate = list(aux_candidate)\n",
        "        can_join = True\n",
        "        for i in range(k - 2):\n",
        "          if candidato[i] != aux_candidate[i]:\n",
        "            can_join = False\n",
        "            break\n",
        "        if not can_join:\n",
        "          continue\n",
        "        if candidato[k - 2] < aux_candidate[k - 2]:\n",
        "          c = candidato + [aux_candidate[k - 2]]\n",
        "          c = frozenset(sorted(c))\n",
        "          C_k.add(c)\n",
        "    return C_k\n",
        "  \n",
        "  \"\"\" obtenemos el min_support de itemset e intersectamos con las playlists \"\"\"\n",
        "  def calculate_subset_count(self, subset):\n",
        "    playlists_inter = []\n",
        "    for song in subset:\n",
        "      playlists_inter.append(self.songs_in_playlists[song])\n",
        "    return len(set.intersection(*playlists_inter))\n",
        "\n",
        "  \"\"\" Nos retorna un conjuntos de itemsets que hayan superado el min_support\"\"\"\n",
        "  def podar_itemsets(self, C_k):\n",
        "    C_k_counter = {}\n",
        "    playlist_length = len(self.playlists)\n",
        "    for candidate in C_k:\n",
        "      C_k_counter[candidate] = self.calculate_subset_count(candidate)\n",
        "    L_k_counter = { subset: times for subset, times in C_k_counter.items() if times / playlist_length >= self.min_support}\n",
        "    return L_k_counter\n",
        "\n",
        "  \"\"\" Esta funcion nos permite exportar los itemsets frecuentes a csv, para\n",
        "  poder verlo con facilidad y mas legible\"\"\"\n",
        "  def export_frequent_itemsets_to_csv(self):\n",
        "    self.frequent_itemsets_df = pd.DataFrame([item for sublist in self.k_frequent_itemsets.values() for item in sublist]).round(3)\n",
        "    self.frequent_itemsets_df.columns = [\"itemset\", \"count_support\"]\n",
        "    self.frequent_itemsets_df[\"support\"] = self.frequent_itemsets_df[\"count_support\"] / len(self.playlists) # support\n",
        "    print(\"Generando archivo de itemsets frecuentes.\") # mostramos mensaje de informacion\n",
        "    self.frequent_itemsets_df.to_csv(\"itemsets_frecuentes.csv\", index=\"False\") # guardamos los itemsets frecuentes en el archivo.csv\n",
        "    return self.frequent_itemsets_df\n",
        "\n",
        "  \"\"\" Este método nos genera los itemsets frecuentes\"\"\"\n",
        "  def get_frequent_itemsets(self):\n",
        "    self.preparar_datos() #Preparamos los datos de la playlists\n",
        "    self.obtener_apariciones_canciones() # obtenemos itemsets frecuentes\n",
        "    self.generate_L_1() # generamos las itemsets que superen el min_support\n",
        "    self.k_frequent_itemsets = {}\n",
        "    self.k_frequent_itemsets[1] = sorted(self.L_1_counter.items(), key=lambda x: x[1], reverse=True)\n",
        "    self.frequent_itemsets_length_gt2 = []\n",
        "    k = 2\n",
        "    current = self.L_1\n",
        "    while len(current) != 0:\n",
        "      C_k = self.posibles_itemsets_frecuentes(current, k)\n",
        "      L_k_counter = self.podar_itemsets(C_k)\n",
        "      L_k = L_k_counter.keys()\n",
        "      self.frequent_itemsets_length_gt2.extend(L_k)\n",
        "      self.k_frequent_itemsets[k] = sorted(\n",
        "        L_k_counter.items(), key=lambda x: x[1], reverse=True)\n",
        "      k += 1\n",
        "      current = L_k\n",
        "    self.export_frequent_itemsets_to_csv()\n",
        "\n",
        "  \"\"\"Nos forma todas las combianciones de un itemset\"\"\"\n",
        "  def get_association_rule_from_itemset(self, itemset):\n",
        "    itemset_count = self.calculate_subset_count(itemset)\n",
        "    itemset_support = itemset_count / len(self.playlists)\n",
        "    for i in range(1, len(itemset) + 1):\n",
        "      for x_set in combinaciones(itemset, i):\n",
        "        x_set = set(x_set)\n",
        "        y_set = set(itemset) - x_set\n",
        "        x_support = self.calculate_subset_count(x_set) / len(\n",
        "          self.playlists)\n",
        "        x_y_support = self.calculate_subset_count(\n",
        "          x_set.union(y_set)) / len(self.playlists)\n",
        "        rule_confidence = x_y_support / x_support\n",
        "        if len(x_set) > 0 and len(y_set) > 0:\n",
        "          y_support = self.calculate_subset_count(y_set) / len(\n",
        "            self.playlists)\n",
        "          rule_lift = x_y_support / (x_support * y_support)\n",
        "          self.rules.append((x_set, y_set, rule_confidence, x_y_support, rule_lift))\n",
        "\n",
        "  \"\"\" Este modulo genera todas las reglas de asociaciion posibles, todo esto a partir \n",
        "  de los itemsets frecuentes\"\"\"\n",
        "  def generate_association_rules(self):\n",
        "    self.rules = [] # inicializamos un arreglo vacio\n",
        "    for itemset in self.frequent_itemsets_length_gt2: \n",
        "      self.get_association_rule_from_itemset(itemset) # obtenemos las reglas de asociacion para cada itemset\n",
        "    self.exportar_reglas() # Exportamos las reglas de asociacion\n",
        "\n",
        "  \"\"\" Este modulo nos sirve pare exportar las reglas de asociacion al formato csv utilizando pandas\"\"\"\n",
        "  def exportar_reglas(self):\n",
        "    self.rules_df = pd.DataFrame(data=self.rules, columns=[\"anterior\", \"siguiente\", \"confidence\", \"support\", \"lift\"]).round(3) # creamos un data frame con las etiquetas\n",
        "    print(\"Generando archivo de reglas de asociacion.\") # Mensaje de informacion\n",
        "    self.rules_df[\"anterior\"] = list(map(tuple, self.rules_df[\"anterior\"])) # mapeamos los anteriores\n",
        "    self.rules_df[\"siguiente\"] = list(map(tuple, self.rules_df[\"siguiente\"])) #mapeamos los siguientes\n",
        "    self.rules_df.to_csv(\"reglas_de_asociacion.csv\", index=False) # guardamos el data frame en un archivo .csv\n",
        "    return self.rules_df\n",
        "\n",
        "  \"\"\" Este modulo nos reportar los n reglas de asociacion con mayor confidencia, pero podemos cambiar a lift\"\"\"\n",
        "  def obtener_reglas_asociaciacion(self, n, ordenado_por=\"confidence\"):\n",
        "    sorted_data_frame = self.rules_df.sort_values(ordenado_por, ascending=False).head(n) # data frame\n",
        "    print(\"Genrando top {} reglas ordenado por {}.\".format(n, ordenado_por)) # mostrar mensaje de informacion\n",
        "    sorted_data_frame.to_csv(\"top_{}_rule_by_{}.csv\".format(n, ordenado_por), index=False) # generar un archivo .csv del data frame ordenado\n",
        "    return sorted_data_frame\n",
        "\n",
        "  \"\"\"Este modulo nos permite filtrar todas las reglas segun condifence y lift o ambos por separado\"\"\"\n",
        "  def filtrar_reglas(self, filtrar_por=\"ambos\"):\n",
        "    if filtrar_por == \"ambos\": # ambos significa confidence y lift\n",
        "      filtered_rules = self.rules_df[(self.rules_df[\"confidence\"] >= self.confidence)& (self.rules_df[\"lift\"] >= self.lift)]\n",
        "    elif filtrar_por == \"confidence\": # filtrar solo por confidence\n",
        "      filtered_rules = self.rules_df[self.rules_df[\"confidence\"] >= self.confidence].sort_values(filtrar_por, ascending=False)\n",
        "    else:\n",
        "      filtered_rules = self.rules_df[\n",
        "        self.rules_df[\"lift\"] >= self.lift].sort_values(filtrar_por, ascending=False)\n",
        "    display(filtered_rules.head(10)) # filtrar solo 10 reglas\n",
        "    filtered_rules.to_csv(\"reglas_filtradas.csv\", index=False)\n",
        "    print(\"Primeras 1o reglas de asociacion filtradas\")"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnAAq5NzOeqX"
      },
      "source": [
        "Cargar los datos de las playlists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I_znuzHv645"
      },
      "source": [
        "spotify = np.load(\"spotify.npy\", allow_pickle=True)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_inik1OOmLG"
      },
      "source": [
        "Inicializar la clase Apriori para poder usar sus métodos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRqBt4WPwkuC"
      },
      "source": [
        "apriori = Apriori( datos=spotify, min_support=0.01, confidence=0.5, lift=1.2)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsiZngRVO_2_"
      },
      "source": [
        "Primero obtendremos los itemsets frecuentes para poder obtener las reglas de asociacion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWXvNm46PIRp",
        "outputId": "a848fc7a-67f2-41bf-e7cf-be182875be18"
      },
      "source": [
        "apriori.get_frequent_itemsets()"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generando archivo de itemsets frecuentes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTJsvikaPr2P"
      },
      "source": [
        "Ahora procedemos a generar las reglas de asociacion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTmxW3OuPrSn",
        "outputId": "4520fc15-777b-4d05-b972-85968351c5d0"
      },
      "source": [
        "apriori.generate_association_rules()"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generando archivo de reglas de asociacion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTWkYAs-R_t_"
      },
      "source": [
        "Finalmente mostramos las 10 primeras de asociacion, para ello tenemos que pasar de parametro n = 10, y las reglas de asociacion estan ordenadas según la confianza para saber la probabilidad que que la musica siguiente se escuche, una vez terminado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "OJT2qghPSMDV",
        "outputId": "6d71b61d-f79b-4b59-a39d-8877bfb82d79"
      },
      "source": [
        "apriori.obtener_reglas_asociaciacion(10)\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genrando top 10 reglas ordenado por confidence.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>anterior</th>\n",
              "      <th>siguiente</th>\n",
              "      <th>confidence</th>\n",
              "      <th>support</th>\n",
              "      <th>lift</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>(Mask Off, DNA.)</td>\n",
              "      <td>(HUMBLE.,)</td>\n",
              "      <td>0.909</td>\n",
              "      <td>0.010</td>\n",
              "      <td>19.550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>(XO TOUR Llif3, DNA.)</td>\n",
              "      <td>(HUMBLE.,)</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.010</td>\n",
              "      <td>18.589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>(DNA.,)</td>\n",
              "      <td>(HUMBLE.,)</td>\n",
              "      <td>0.823</td>\n",
              "      <td>0.019</td>\n",
              "      <td>17.688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>(Mask Off, XO TOUR Llif3)</td>\n",
              "      <td>(HUMBLE.,)</td>\n",
              "      <td>0.804</td>\n",
              "      <td>0.013</td>\n",
              "      <td>17.283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>(Broccoli (feat. Lil Yachty), Bounce Back)</td>\n",
              "      <td>(Bad and Boujee (feat. Lil Uzi Vert),)</td>\n",
              "      <td>0.775</td>\n",
              "      <td>0.010</td>\n",
              "      <td>22.469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>(XO TOUR Llif3, Slippery (feat. Gucci Mane))</td>\n",
              "      <td>(HUMBLE.,)</td>\n",
              "      <td>0.765</td>\n",
              "      <td>0.010</td>\n",
              "      <td>16.455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>(XO TOUR Llif3, Tunnel Vision)</td>\n",
              "      <td>(HUMBLE.,)</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.010</td>\n",
              "      <td>16.129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>(Mask Off, Congratulations)</td>\n",
              "      <td>(HUMBLE.,)</td>\n",
              "      <td>0.747</td>\n",
              "      <td>0.012</td>\n",
              "      <td>16.063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>(Mask Off, Bounce Back)</td>\n",
              "      <td>(HUMBLE.,)</td>\n",
              "      <td>0.743</td>\n",
              "      <td>0.010</td>\n",
              "      <td>15.971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>(Mask Off, goosebumps)</td>\n",
              "      <td>(HUMBLE.,)</td>\n",
              "      <td>0.743</td>\n",
              "      <td>0.011</td>\n",
              "      <td>15.984</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         anterior  ...    lift\n",
              "446                              (Mask Off, DNA.)  ...  19.550\n",
              "410                         (XO TOUR Llif3, DNA.)  ...  18.589\n",
              "21                                        (DNA.,)  ...  17.688\n",
              "367                     (Mask Off, XO TOUR Llif3)  ...  17.283\n",
              "385    (Broccoli (feat. Lil Yachty), Bounce Back)  ...  22.469\n",
              "415  (XO TOUR Llif3, Slippery (feat. Gucci Mane))  ...  16.455\n",
              "392                (XO TOUR Llif3, Tunnel Vision)  ...  16.129\n",
              "379                   (Mask Off, Congratulations)  ...  16.063\n",
              "421                       (Mask Off, Bounce Back)  ...  15.971\n",
              "397                        (Mask Off, goosebumps)  ...  15.984\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ymICkGaT3tR"
      },
      "source": [
        "Escogeremis las 4 primeras regla.\n",
        "El soporte nos indica la frecuencia en que la musica aparece en las playlists.La confianza nos indica la probabilidad de que escuchemos la cancion siguiente.\n",
        "Y el lift la probabilidad de que escuchemos una cancion siguiente, uan vez escuchada la cancion actual dependiendo de la popularidad de la música."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "qV6jLRUNTyFv",
        "outputId": "124b63d0-e377-4121-af46-aa9af81d12c9"
      },
      "source": [
        "apriori.obtener_reglas_asociaciacion(4)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genrando top 4 reglas ordenado por confidence.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>anterior</th>\n",
              "      <th>siguiente</th>\n",
              "      <th>confidence</th>\n",
              "      <th>support</th>\n",
              "      <th>lift</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>(Mask Off, DNA.)</td>\n",
              "      <td>(HUMBLE.,)</td>\n",
              "      <td>0.909</td>\n",
              "      <td>0.010</td>\n",
              "      <td>19.550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>(XO TOUR Llif3, DNA.)</td>\n",
              "      <td>(HUMBLE.,)</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.010</td>\n",
              "      <td>18.589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>(DNA.,)</td>\n",
              "      <td>(HUMBLE.,)</td>\n",
              "      <td>0.823</td>\n",
              "      <td>0.019</td>\n",
              "      <td>17.688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>(Mask Off, XO TOUR Llif3)</td>\n",
              "      <td>(HUMBLE.,)</td>\n",
              "      <td>0.804</td>\n",
              "      <td>0.013</td>\n",
              "      <td>17.283</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      anterior   siguiente  confidence  support    lift\n",
              "446           (Mask Off, DNA.)  (HUMBLE.,)       0.909    0.010  19.550\n",
              "410      (XO TOUR Llif3, DNA.)  (HUMBLE.,)       0.864    0.010  18.589\n",
              "21                     (DNA.,)  (HUMBLE.,)       0.823    0.019  17.688\n",
              "367  (Mask Off, XO TOUR Llif3)  (HUMBLE.,)       0.804    0.013  17.283"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD1wHA2xf-ZD"
      },
      "source": [
        "\"\"\"def generate_association_rules(frequent_itemsets, confidence = 0, lift=0):\n",
        "  support = []\n",
        "  m = len(frequent_itemsets)\n",
        "  for item in frequent_itemsets[1]:\n",
        "    support_aux = []\n",
        "    n = len(item)\n",
        "    for i in range(item):\n",
        "      support_aux.append(item[i] / frequent_itemsets[2])\n",
        "    support.append(support_aux)\n",
        "\n",
        "  items_frequents = frequent_itemsets[0][m-1]\n",
        "  n_itemsFreq = len(items_frequents)\n",
        "  for i in range(n_itemsFreq):\n",
        "    print(\"Item: \", items_frequents[1])\n",
        "    rules_itemsets = generate_rules(items_frequents[i])\n",
        "    rules_lift = []\n",
        "    rules_confidence = [] \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}